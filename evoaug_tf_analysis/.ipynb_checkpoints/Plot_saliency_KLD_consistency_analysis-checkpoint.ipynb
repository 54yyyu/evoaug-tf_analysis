{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d355b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from six.moves import cPickle\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import evoaug_tf\n",
    "from evoaug_tf import evoaug, augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepSTARR(input_shape, output_shape=2):\n",
    "\n",
    "    # body\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = keras.layers.Conv1D(256, kernel_size=7, padding='same', name='Conv1D_1st')(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = keras.layers.Conv1D(60, kernel_size=3, padding='same', name=str('Conv1D_2'))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = keras.layers.Conv1D(60, kernel_size=5, padding='same', name=str('Conv1D_3'))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = keras.layers.Conv1D(120, kernel_size=3, padding='same', name=str('Conv1D_4'))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    \n",
    "    # dense layers\n",
    "    x = keras.layers.Dense(256, name=str('Dense_1'))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = keras.layers.Dense(256, name=str('Dense_2'))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # heads per task (developmental and housekeeping enhancer activities)\n",
    "    outputs = keras.layers.Dense(output_shape, activation='linear')(x) #tasks = ['Dev', 'Hk']\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def _pad_end(x, insert_max):\n",
    "    \"\"\"Add random DNA padding of length insert_max to the end of each sequence in batch.\"\"\"\n",
    "    N = tf.shape(x)[0]\n",
    "    L = tf.shape(x)[1]\n",
    "    A = tf.cast(tf.shape(x)[2], dtype = tf.float32)\n",
    "\n",
    "    a = tf.eye(A)\n",
    "    p = tf.ones((A,)) / A\n",
    "    padding = tf.transpose(tf.gather(a, tf.random.categorical(tf.math.log([p] * insert_max), N)), perm=[1,0,2])\n",
    "    x_padded = tf.concat([x, padding], axis=1)\n",
    "    return x_padded\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def saliency_map(X, model, class_index=0):\n",
    "\n",
    "    if not tf.is_tensor(X):\n",
    "        X = tf.Variable(X)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X)\n",
    "        X = _pad_end(X, 20)\n",
    "        outputs = model.model(X)[:, class_index]\n",
    "    grad = tape.gradient(outputs, X)\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b92aa",
   "metadata": {},
   "source": [
    "# ACME functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2632ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax, kl_div, rel_entr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "PI = 3.1416\n",
    "\n",
    "# Local prior entropy \n",
    "def ortonormal_coordinates(attr_map):\n",
    "    \"\"\"reduce 4d array to 3d\"\"\"\n",
    "\n",
    "    attr_map_on = np.zeros((attr_map.shape[0], attr_map.shape[1], 3))\n",
    "\n",
    "    x = attr_map[:, :, 0]\n",
    "    y = attr_map[:, :, 1]\n",
    "    z = attr_map[:, :, 2]\n",
    "    w = attr_map[:, :, 3]\n",
    "\n",
    "    # Now convert to new coordinates\n",
    "    e1 = 1 / np.sqrt(2) * (-x + y)\n",
    "    e2 = np.sqrt(2 / 3) * (-1/2*x -1/2*y)\n",
    "    e3 = np.sqrt(3 / 4) * (-1/3*x -1/3*y -1/3*z + w)\n",
    "    attr_map_on[:, :, 0] = e1\n",
    "    attr_map_on[:, :, 1] = e2\n",
    "    attr_map_on[:, :, 2] = e3\n",
    "\n",
    "    return attr_map_on\n",
    "\n",
    "def process_attribution_map(saliency_map_raw):\n",
    "    saliency_map_raw = saliency_map_raw - np.mean(saliency_map_raw, axis=-1, keepdims=True) # gradient correction\n",
    "    saliency_map_raw = saliency_map_raw / np.sum(np.sqrt(np.sum(np.square(saliency_map_raw), axis=-1, keepdims=True)), axis=-2, keepdims=True) #normalize\n",
    "    saliency_map_raw_rolled = np.roll(saliency_map_raw, -1, axis=-2)\n",
    "    saliency_map_raw_rolled_twice = np.roll(saliency_map_raw, -2, axis=-2)\n",
    "    saliency_map_raw_rolled_triple = np.roll(saliency_map_raw, -3, axis=-2)\n",
    "    saliency_map_raw_rolled_4 = np.roll(saliency_map_raw, -4, axis=-2)\n",
    "    saliency_map_raw_rolled_5 = np.roll(saliency_map_raw, -5, axis=-2)\n",
    "    saliency_map_raw_rolled_6 = np.roll(saliency_map_raw, -6, axis=-2)\n",
    "    # Define k-window here, include k terms below (here k = 3)\n",
    "    saliency_special = saliency_map_raw + saliency_map_raw_rolled + saliency_map_raw_rolled_twice #+ saliency_map_raw_rolled_triple # + saliency_map_raw_rolled_4 + saliency_map_raw_rolled_5 #This line is optional.\n",
    "    saliency_special = ortonormal_coordinates(saliency_special) #Down to 3D, since data lives on the plane.\n",
    "    return saliency_special\n",
    "\n",
    "def spherical_coordinates_process_2_trad(saliency_map_raw_s, X, mask, radius_count_cutoff=0.04):   # r_squared_weighted\n",
    "    global N_EXP\n",
    "    N_EXP = len(saliency_map_raw_s)\n",
    "    radius_count=int(radius_count_cutoff * np.prod(X.shape)/4)\n",
    "    cutoff=[]\n",
    "    x_s, y_s, z_s, r_s, phi_1_s, phi_2_s = [], [], [], [], [], []\n",
    "    for s in range (0, N_EXP):\n",
    "        saliency_map_raw = saliency_map_raw_s[s]\n",
    "        xxx_motif=saliency_map_raw[:,:,0]\n",
    "        yyy_motif=(saliency_map_raw[:,:,1])\n",
    "        zzz_motif=(saliency_map_raw[:,:,2])\n",
    "        xxx_motif_pattern=saliency_map_raw[:,:,0]*mask\n",
    "        yyy_motif_pattern=(saliency_map_raw[:,:,1])*mask\n",
    "        zzz_motif_pattern=(saliency_map_raw[:,:,2])*mask\n",
    "        r=np.sqrt(xxx_motif*xxx_motif+yyy_motif*yyy_motif+zzz_motif*zzz_motif)\n",
    "        resh = X.shape[0] * X.shape[1]\n",
    "        x=np.array(xxx_motif_pattern.reshape(resh,))\n",
    "        y=np.array(yyy_motif_pattern.reshape(resh,))\n",
    "        z=np.array(zzz_motif_pattern.reshape(resh,))\n",
    "        r=np.array(r.reshape(resh,))\n",
    "        #Take care of any NANs.\n",
    "        x=np.nan_to_num(x)\n",
    "        y=np.nan_to_num(y)\n",
    "        z=np.nan_to_num(z)\n",
    "        r=np.nan_to_num(r)\n",
    "        cutoff.append( np.sort(r)[-radius_count] )\n",
    "        R_cuttof_index = np.sqrt(x*x+y*y+z*z) > cutoff[s]\n",
    "        #Cut off\n",
    "        x=x[R_cuttof_index]\n",
    "        y=y[R_cuttof_index]\n",
    "        z=z[R_cuttof_index]\n",
    "        r=np.array(r[R_cuttof_index])\n",
    "        x_s.append(x)\n",
    "        y_s.append(y)\n",
    "        z_s.append(z)\n",
    "        r_s.append(r)\n",
    "        #rotate axis\n",
    "        x__ = np.array(y)\n",
    "        y__ = np.array(z)\n",
    "        z__ = np.array(x)\n",
    "        x = x__\n",
    "        y = y__\n",
    "        z = z__\n",
    "        #\"phi\"\n",
    "        phi_1 = np.arctan(y/x) #default\n",
    "        phi_1 = np.where((x<0) & (y>=0), np.arctan(y/x) + PI, phi_1)   #overwrite\n",
    "        phi_1 = np.where((x<0) & (y<0), np.arctan(y/x) - PI, phi_1)   #overwrite\n",
    "        phi_1 = np.where (x==0, PI/2, phi_1) #overwrite\n",
    "        #Renormalize temorarily to have both angles in [0,PI]:\n",
    "        phi_1 = phi_1/2 + PI/2\n",
    "        #\"theta\"\n",
    "        phi_2=np.arccos(z/r)\n",
    "        #back to list\n",
    "        phi_1 = list(phi_1)\n",
    "        phi_2 = list(phi_2)\n",
    "        phi_1_s.append(phi_1)\n",
    "        phi_2_s.append(phi_2)\n",
    "    #print(cutoff)\n",
    "    return phi_1_s, phi_2_s, r_s\n",
    "\n",
    "def initialize_integration_2(box_length):\n",
    "    LIM = 3.1416\n",
    "    global volume_border_correction\n",
    "    box_volume = box_length*box_length\n",
    "    n_bins = int(LIM/box_length)\n",
    "    volume_border_correction =  (LIM/box_length/n_bins)*(LIM/box_length/n_bins)\n",
    "    #print('volume_border_correction = ', volume_border_correction)\n",
    "    n_bins_half = int(n_bins/2)\n",
    "    return LIM, box_length, box_volume, n_bins, n_bins_half\n",
    "\n",
    "def Empiciral_box_pdf_func_2 (phi_1, phi_2, r_s, n_bins, box_length, box_volume):\n",
    "    N_points = len(phi_1) #Number of points\n",
    "    Empirical_box_count = np.zeros((n_bins, n_bins))\n",
    "    Empirical_box_count_plain = np.zeros((n_bins, n_bins))\n",
    "    #Now populate the box. Go over every single point.\n",
    "    for i in range (0, N_points):\n",
    "        # k, l are box numbers of the (phi_1, phi_2) point\n",
    "        k=np.minimum(int(phi_1[i]/box_length), n_bins-1)\n",
    "        l=np.minimum(int(phi_2[i]/box_length), n_bins-1)\n",
    "        #Increment count in (k,l,m) box:\n",
    "        Empirical_box_count[k,l]+=1*r_s[i]*r_s[i]\n",
    "        Empirical_box_count_plain[k,l]+=1\n",
    "    #To get the probability distribution, divide the Empirical_box_count by the total number of points.\n",
    "    Empirical_box_pdf = Empirical_box_count / N_points / box_volume\n",
    "    #Check that it integrates to around 1:\n",
    "    #print('Integral of the empirical_box_pdf (before first renormalization) = ' , np.sum(Empirical_box_pdf*box_volume), '(should be 1.0 if OK) \\n')\n",
    "    correction = 1 / np.sum(Empirical_box_pdf*box_volume)\n",
    "    #Another, optional correction \n",
    "    count_empty_boxes = 0\n",
    "    count_single_points = 0\n",
    "    for k in range (1, n_bins-1):\n",
    "        for l in range(1,n_bins-1):\n",
    "            if(Empirical_box_count[k,l] ==1):\n",
    "                count_empty_boxes+=1\n",
    "                count_single_points+=1\n",
    "    return Empirical_box_pdf * correction * 1 , Empirical_box_count *correction , Empirical_box_count_plain #, correction2\n",
    "\n",
    "\n",
    "def KL_divergence_2(Empirical_box_pdf, Empirical_box_count, Empirical_box_count_plain, n_bins, box_volume, prior_range):  #, correction2)\n",
    "    # p= empirical distribution, q=prior spherical distribution\n",
    "    # Notice that the prior distribution is never 0! So it is safe to divide by q.\n",
    "    # L'Hospital rule provides that p*log(p) --> 0 when p->0. When we encounter p=0, we would just set the contribution of that term to 0, i.e. ignore it in the sum.\n",
    "    Relative_entropy = 0\n",
    "    for i in range (1, n_bins-1):\n",
    "        for j in range(1,n_bins-1):\n",
    "            if (Empirical_box_pdf[i,j] > 0  ):\n",
    "                phi_1 = i/n_bins*PI\n",
    "                phi_2 = j/n_bins*PI\n",
    "                correction3 = 0\n",
    "                prior_counter = 0\n",
    "                prior=0\n",
    "                for ii in range(-prior_range,prior_range):\n",
    "                    for jj in range(-prior_range,prior_range):\n",
    "                        if(i+ii>0 and i+ii<n_bins and j+jj>0 and j+jj<n_bins):\n",
    "                            prior+=Empirical_box_pdf[i+ii,j+jj]\n",
    "                            prior_counter+=1\n",
    "                prior=prior/prior_counter\n",
    "                if(prior>0) : KL_divergence_contribution = Empirical_box_pdf[i,j] * np.log (Empirical_box_pdf[i,j]  /  prior )\n",
    "                if(np.sin(phi_1)>0 and prior>0 ): Relative_entropy+=KL_divergence_contribution  #and Empirical_box_count_plain[i,j]>1\n",
    "    Relative_entropy = Relative_entropy * box_volume #(volume differential in the \"integral\")\n",
    "    return np.round(Relative_entropy,3)\n",
    "\n",
    "def calculate_entropy_2(phi_1_s, phi_2_s, r_s, n_bins, box_length, box_volume, prior_range):\n",
    "    global Empirical_box_pdf_s\n",
    "    global Empirical_box_count_s\n",
    "    global Empirical_box_count_plain_s\n",
    "    Empirical_box_pdf_s=[]\n",
    "    Empirical_box_count_s = []\n",
    "    Empirical_box_count_plain_s = []\n",
    "    prior_correction_s = []\n",
    "    Spherical_box_prior_pdf_s=[]\n",
    "    for s in range (0,N_EXP):\n",
    "        #print(s)\n",
    "        Empirical_box_pdf_s.append(Empiciral_box_pdf_func_2(phi_1_s[s],phi_2_s[s], r_s[s], n_bins, box_length, box_volume)[0])\n",
    "        Empirical_box_count_s.append(Empiciral_box_pdf_func_2(phi_1_s[s],phi_2_s[s], r_s[s], n_bins, box_length, box_volume)[1])\n",
    "        Empirical_box_count_plain_s.append(Empiciral_box_pdf_func_2(phi_1_s[s],phi_2_s[s], r_s[s], n_bins, box_length, box_volume)[2])\n",
    "    Entropic_information = []\n",
    "    for s in range (0,N_EXP):\n",
    "        Entropic_information.append ( KL_divergence_2 (Empirical_box_pdf_s[s], Empirical_box_count_s[s], Empirical_box_count_plain_s[s], n_bins, box_volume, prior_range)  )\n",
    "    return list(Entropic_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee00f691",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e77cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_deepstarr_data(\n",
    "        data_split: str,\n",
    "        data_dir='DeepSTARR_data.h5',\n",
    "        subsample: bool = False\n",
    "    ) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"Load dataset\"\"\"\n",
    "\n",
    "    # load sequences and labels\n",
    "    with h5py.File(data_dir, \"r\") as dataset:\n",
    "        x = np.array(dataset[f'X_{data_split}']).astype(np.float32).transpose([0,2,1])\n",
    "        y = np.array(dataset[f'Y_{data_split}']).astype(np.float32)\n",
    "    if subsample:\n",
    "        if data_split == \"train\":\n",
    "            x = x[:80000]\n",
    "            y = y[:80000]\n",
    "        elif data_split == \"valid\":\n",
    "            x = x[:20000]\n",
    "            y = y[:20000]\n",
    "        else:\n",
    "            x = x[:10000]\n",
    "            y = y[:10000]\n",
    "    return x, y\n",
    "\n",
    "x_test, y_test = load_deepstarr_data(data_split=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8705f8c",
   "metadata": {},
   "source": [
    "# Generate saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40f5721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 21:02:50.697155: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-05 21:02:51.393509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9649 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:40:00.0, compute capability: 7.5\n",
      "2024-01-05 21:02:53.209439: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100\n",
      "2024-01-05 21:02:53.803502: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.382\n",
      "0 1 0.419\n",
      "1 0 0.406\n",
      "1 1 0.415\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function saliency_map at 0x7f469141c1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2 0 0.371\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function saliency_map at 0x7f469141c1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2 1 0.375\n",
      "3 0 0.438\n",
      "3 1 0.436\n",
      "4 0 0.412\n",
      "4 1 0.409\n"
     ]
    }
   ],
   "source": [
    "working_dir = \".\"\n",
    "\n",
    "# create list of model runs\n",
    "hits = [\n",
    "    \"../results/deepstarr_finetune_0.h5\",\n",
    "    \"../results/deepstarr_finetune_1.h5\",\n",
    "    \"../results/deepstarr_finetune_2.h5\",\n",
    "    \"../results/deepstarr_finetune_3.h5\",\n",
    "    \"../results/deepstarr_finetune_4.h5\",\n",
    "]\n",
    "\n",
    "augment_list = [\n",
    "    augment.RandomDeletion(delete_min=0, delete_max=30),\n",
    "    augment.RandomRC(rc_prob=0.5),\n",
    "    augment.RandomInsertion(insert_min=0, insert_max=20),\n",
    "    augment.RandomDeletion(delete_min=0, delete_max=30),\n",
    "    augment.RandomTranslocation(shift_min=0, shift_max=20),\n",
    "    augment.RandomNoise(noise_mean=0, noise_std=0.3),\n",
    "    augment.RandomMutation(mutate_frac=0.05)\n",
    "]\n",
    "\n",
    "\n",
    "results_evoaug = []\n",
    "for index in range(5):\n",
    "    tmp_results = []\n",
    "    for class_index in [0,1]:\n",
    "        _, L, A = x_test.shape\n",
    "        model = evoaug.RobustModel(model_zoo.DeepSTARR, input_shape=(L,A), augment_list=augment_list, max_augs_per_seq=1, hard_aug=True)\n",
    "\n",
    "        model.compile(keras.optimizers.Adam(learning_rate=0.001), #weight_decay\n",
    "                    loss='mse', metrics=[Spearman, pearson_r]) # additional track metric    \n",
    "        model.load_weights(hits[index])\n",
    "\n",
    "\n",
    "        sort_index = np.argsort(y_test[:,class_index])[::-1]\n",
    "        X = x_test[sort_index[:200]]\n",
    "\n",
    "        saliency_scores = saliency_map(X, model, class_index=0)\n",
    "        #saliency_scores = saliency_scores[:,:L,:]\n",
    "\n",
    "        N, L, A = saliency_scores.shape\n",
    "        X2 = _pad_end(X, insert_max=20)\n",
    "\n",
    "        # hyperparameters for ACME functions\n",
    "        radius_count_cutoff = 0.10\n",
    "        box_length = 0.1\n",
    "        attribution_map = process_attribution_map(saliency_scores)\n",
    "        unit_mask = np.sum(np.ones(X2.shape),axis=-1)/4\n",
    "        phi_1_s, phi_2_s, r_s = spherical_coordinates_process_2_trad([attribution_map], X2, unit_mask, radius_count_cutoff)\n",
    "        LIM, box_length, box_volume, n_bins, n_bins_half = initialize_integration_2(box_length)\n",
    "        entropic_information = calculate_entropy_2(phi_1_s, phi_2_s, r_s, n_bins, box_length, box_volume, prior_range=3)\n",
    "\n",
    "        print(index, class_index, entropic_information[0])\n",
    "        \n",
    "        tmp_results.append(entropic_information[0])\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    \n",
    "    results_evoaug.append(tmp_results)\n",
    "results_evoaug = np.array(results_evoaug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9feede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.343\n",
      "0 1 0.296\n",
      "1 0 0.36\n",
      "1 1 0.286\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function saliency_map at 0x7f45cb100f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2 0 0.363\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function saliency_map at 0x7f45cb100f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2 1 0.295\n",
      "3 0 0.3\n",
      "3 1 0.295\n",
      "4 0 0.305\n",
      "4 1 0.279\n"
     ]
    }
   ],
   "source": [
    "working_dir = \".\"\n",
    "\n",
    "# create list of model runs\n",
    "hits = [\n",
    "    \"../results/standard_0.h5\",\n",
    "    \"../results/standard_1.h5\",\n",
    "    \"../results/standard_2.h5\",\n",
    "    \"../results/standard_3.h5\",\n",
    "    \"../results/standard_4.h5\",\n",
    "]\n",
    "\n",
    "results_standard = []\n",
    "for index in range(5):\n",
    "    tmp_results = []\n",
    "    for class_index in [0,1]:\n",
    "        _, L, A = x_test.shape\n",
    "        model = model_zoo.DeepSTARR(input_shape=(L,A))\n",
    "        model.compile(keras.optimizers.Adam(learning_rate=0.001), #weight_decay\n",
    "                    loss='mse', metrics=[Spearman, pearson_r]) # additional track metric    \n",
    "        model.load_weights(hits[index])\n",
    "\n",
    "        sort_index = np.argsort(y_test[:,class_index])[::-1]\n",
    "        X = x_test[sort_index[:200]]\n",
    "\n",
    "        saliency_scores = saliency_map(X, model, class_index=0)\n",
    "        #saliency_scores = saliency_scores[:,:L,:]\n",
    "\n",
    "        N, L, A = saliency_scores.shape\n",
    "        X2 = X\n",
    "\n",
    "        # hyperparameters for ACME functions\n",
    "        radius_count_cutoff = 0.10\n",
    "        box_length = 0.1\n",
    "        attribution_map = process_attribution_map(saliency_scores)\n",
    "        unit_mask = np.sum(np.ones(X2.shape),axis=-1)/4\n",
    "        phi_1_s, phi_2_s, r_s = spherical_coordinates_process_2_trad([attribution_map], X2, unit_mask, radius_count_cutoff)\n",
    "        LIM, box_length, box_volume, n_bins, n_bins_half = initialize_integration_2(box_length)\n",
    "        entropic_information = calculate_entropy_2(phi_1_s, phi_2_s, r_s, n_bins, box_length, box_volume, prior_range=3)\n",
    "\n",
    "        print(index, class_index, entropic_information[0])\n",
    "\n",
    "        tmp_results.append(entropic_information[0])\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    results_standard.append(tmp_results)\n",
    "results_standard = np.array(results_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5847e974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.343, 0.296],\n",
       "       [0.36 , 0.286],\n",
       "       [0.363, 0.295],\n",
       "       [0.3  , 0.295],\n",
       "       [0.305, 0.279]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eea86d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.382, 0.419],\n",
       "       [0.406, 0.415],\n",
       "       [0.371, 0.375],\n",
       "       [0.438, 0.436],\n",
       "       [0.412, 0.409]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_evoaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78ad1e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "fontsize = 8\n",
    "\n",
    "cmap = cm.ScalarMappable(cmap='tab10')\n",
    "rgb = cmap.to_rgba(np.linspace(0, 1, 8))\n",
    "\n",
    "fig = plt.figure(figsize=(3, 2.5))\n",
    "\n",
    "bplot1 = plt.boxplot([results_standard[:,0]],\n",
    "                     notch=False, patch_artist=True, widths=0.22, positions=[1-.2],\n",
    "                     medianprops=dict(color=\"black\", linewidth=1), showfliers=False)\n",
    "\n",
    "bplot2 = plt.boxplot([results_evoaug[:,0]],\n",
    "                     notch=False, patch_artist=True, widths=0.22, positions=[1+.2],\n",
    "                     medianprops=dict(color=\"black\", linewidth=1), showfliers=False)\n",
    "\n",
    "for patch in bplot1['boxes']:\n",
    "    patch.set_facecolor(rgb[0,:])\n",
    "\n",
    "for patch in bplot2['boxes']:\n",
    "    patch.set_facecolor(rgb[1,:])\n",
    "    \n",
    "\n",
    "x1 = .8\n",
    "x2 = 1.2\n",
    "y = .45\n",
    "h = .005\n",
    "plt.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=.8, c='k')\n",
    "plt.text((x1+x2)*.5, y+h, \"***\", ha='center', va='bottom', color='k', fontsize=fontsize)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bplot1 = plt.boxplot([results_standard[:,1]],\n",
    "                     notch=False, patch_artist=True, widths=0.22, positions=[2-.2],\n",
    "                     medianprops=dict(color=\"black\", linewidth=1), showfliers=False)\n",
    "\n",
    "bplot2 = plt.boxplot([results_evoaug[:,1]],\n",
    "                     notch=False, patch_artist=True, widths=0.22, positions=[2+.2],\n",
    "                     medianprops=dict(color=\"black\", linewidth=1), showfliers=False)\n",
    "\n",
    "for patch in bplot1['boxes']:\n",
    "    patch.set_facecolor(rgb[0,:])\n",
    "\n",
    "for patch in bplot2['boxes']:\n",
    "    patch.set_facecolor(rgb[1,:])\n",
    "    \n",
    "\n",
    "x1 = 1.8\n",
    "x2 = 2.2\n",
    "y = .43\n",
    "h = .005\n",
    "plt.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=.8, c='k')\n",
    "plt.text((x1+x2)*.5, y+h, \"***\", ha='center', va='bottom', color='k', fontsize=fontsize)\n",
    "\n",
    "\n",
    "ax = plt.gca();\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=fontsize)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=fontsize)\n",
    "\n",
    "#ax.legend([bplot1[\"boxes\"][0], bplot2[\"boxes\"][0]], ['Standard', 'EvoAug'], loc='upper center', bbox_to_anchor=(0.5, 1.15),\n",
    "#          ncol=3, fontsize=fontsize, frameon=False)\n",
    "\n",
    "plt.ylabel('KLD (k-attr-mean)', fontsize=fontsize)\n",
    "plt.xticks([1,2], ['Dev', 'Hk'], fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "plt.savefig('kld_saliency_comparison.pdf', format='pdf', dpi=200, bbox_inches='tight')\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6858755b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
